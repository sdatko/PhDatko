\section{Open-set classification}
\label{section:open-set-classification}

Traditional, closed-set classification algorithm is designed to recognize data and assign a~label corresponding to one of the classes (categories) known during the model training. Such algorithm can perform exceptionally well, achieving superhuman accuracy in the task, as long as the input data truly correspond to the trained categories. However, when such model is exposed to new, unexpected, previously unseen data categories, it~still assigns one of the originally known labels, resulting in incorrect outcome.

The open-set classification aims to resolve the problem by extending the model's capabilities with additional output category: open-set, indicating that the analyzed input data are not similar enough to any previously known data class. Such ability is especially important for any practical scenarios of Machine Learning models applications, as in real-world the new data points can emerge constantly. This task is commonly known in literature also as the outlier detection, the open-world recognition and the out-of-distribution detection. Many methods were proposed in literature to solve this problem, summarized in surveys \cite{Hodge-2004}\cite{Chandola-2009}.

Gecently, with huge success of Deep Learning models for image and text recognition tasks, a~dedicated line of literature appeared to resolve the problem of open-set classification / out-of-distribution detection in high-dimensional representations generated by such models. Many methods were proposed as state-of-the-art algorithms: Mahalanobis distance with pooled covariance matrix \cite{Lee-2018}, k-Nearest Neighbors \cite{Sun-2022}, Integrated Rank-Weighted Depth \cite{Colombo-2022}, Energy-based OOD detector \cite{Liu-2020},
training with outlier exposure \cite{Hendrycks-2019}, Virtual-logit Matching \cite{Wang-2022} and many others, summarized and compared in recent comprehensive surveys \cite{Geng-2021}\cite{Yang-2022}.

However, although multiple solutions were proposed, the problem remains unresolved, as there is no clear recommendation as to which method should be used in a~practical application –~and the benchmarks present contradicting results \cite{Tajwar-2021}\cite{Yang-2022}. Additionally, the OOD detection in high-dimensional data is highly unstable, as shown in recent publication \cite{Szyc-2023}. Hence, this dissertation is focused on exploring the problem of out-of-distribution detection in high-dimensional feature spaces, analyzing performance and properties of OOD detectors, utilizing high-dimensional representations generated by different Deep Learning models. The goal is to provide new insights and recommendations for reliable OOD detection in Deep Learning, which is crucial for safety-critical deployments of AI in real-world.


\subsection{Outlier detection methods}
\label{section:outlier-detection-methods}

There are three main approaches to the problem of out-of-distribution data recognition commonly recognized in the current literature \cite{Yang-2022}: \textit{post-hoc} methods, training-time regularization and training with outlier exposure.

The \textbf{\textit{post-hoc} methods} are a group of techniques that work on the outputs of existing models, e.g., logit layers fed to softmax function \cite{Bridle-1989}, or that involves the feature-space-wise analysis of the data vectors produced by models (i.e., penultimate layers) to identify the abnormal data. The detection of~out-of-distribution data is typically realized with distance-like functions, such as the Mahalanobis distance \cite{Mahalanobis-1936} or Local Outlier Factor \cite{Breunig-2000}. Section \ref{section:measures} contains the detailed description of selected methods. The significant advantage of \textit{post-hoc} methods is their ease of use and universality, as~they are model agnostic – the computations are performed in the feature-space and they require no additional modifications of the models that produce the feature vectors, so it is possible to utilize any existing and pre-trained model, making them applicable for all kind of real-world implementations (images, texts, ...). This is especially important nowadays, as the re-training of huge deep-learning models can be too expensive or even impossible for practical solutions –~hence the \textit{post-hoc} methods offer an effective solution. Therefore, the \textit{post-hoc} methods that work in the feature-space are the main interest of this dissertation.

The alternative approaches are focused on improving the models' ability to detect outliers by either changing the models architecture and/or modifying the training process, which is usually achieved by adjusting the loss function (objective function). The \textbf{training with outlier exposure} assumes that a~large collection of selected outliers is presented to the model that is then trained to minimize the outputs for such examples \cite{Hendrycks-2019}, i.e., effectively not assign any known label the outliers examples. The problem with practical application of such methods is related to impossibility of obtaining dataset of example outliers that can stand for all possible unknown data that may appear in the future. Therefore, in contrast, the \textbf{training-time regularization} algorithms do not require additional outliers examples, instead utilizing techniques such as the contrastive learning \cite{Tack-2020} to generate class representations that are distant from each other. However, the biggest disadvantage of both mentioned approaches is their computational complexity and requirement of preparing (training) the model customized to a~designated scenario, which is difficult and impractical with big deep-learning state of the art models available nowadays.


\subsection{Near OOD vs Far OOD}
\label{section:near-far-ood}

The current literature distinguishes two major subtasks of outliers detection problem – identifying the near OOD data and the far OOD \cite{Winkens-2020}. This distinguishing highlights the concept that in the real-world applications there are data points coming that show various degrees of similarity/differences from the previously known training samples. Intuitively, the distinction between animals such as dogs and wolves is much harder than distinction between animals and plants. While recognition of far OOD examples is considered much easier, dealing with the near OOD samples is still "a major challenge" \cite{Fort-2021} to be resolved. The~near OOD samples are more prone to spurious assignments.

It shall be noticed that, contrary to semantic differences, what is near and what is far for a~human intuition, it~may have an entirely different meaning for the machine learning algorithm, i.e., in~terms of the feature vectors space distances –~because it all depends on the features identified and correlated with classes by a~chosen representation model during the training process. In the effect, such non-obvious relations as presented in \cite{Ribeiro-2016} can be observed, i.e., wolf class may be correlated with the presence of a~snow in the training data.
